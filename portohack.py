# -*- coding: utf-8 -*-
"""portohack.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1JarNVUXayPBEyMvS12KpLdxrsUeYpqJJ

Este script Python tem como objetivo realizar a captura de dados via API do sistema APS ()
para carregamento no sistema DTE ().

O processo envolve as seguintes etapas:
1. Conectar-se à API do sistema APS para extrair os dados necessários.
2. Processar e transformar os dados extraídos para o formato exigido pelo sistema DTE.
3. Carregar os dados transformados no sistema DTE, garantindo que a integração ocorra de maneira eficiente e sem erros.

O código está estruturado para garantir a correta comunicação entre os sistemas, incluindo a manipulação de erros e validação dos dados.
Certifique-se de ter as credenciais apropriadas e o ambiente configurado antes de executar o script.
"""

import pandas as pd
import requests
import json

# Faze 1 - Captar dados da APS via request

def obter_dados_multiplos():
    """
    Função para obter e validar dados de várias APIs e retornar DataFrames.

    Returns:
        dict: Um dicionário onde as chaves são nomes dos DataFrames e os valores são DataFrames contendo os dados.
    """
    urls = {
        'atracacoes_programadas': 'https://intranet.portodesantos.com.br/_json/porto_hoje.asp?tipo=programados2',
        'navios_atracados': 'https://intranet.portodesantos.com.br/_json/porto_hoje.asp?tipo=atracados',
        'navios_fundeados': 'https://intranet.portodesantos.com.br/_json/porto_hoje.asp?tipo=fundeados',
        'resumo_atracacoes': 'https://intranet.portodesantos.com.br/_json/porto_hoje.asp?tipo=resumo',
        'navios_passageiros': 'https://intranet.portodesantos.com.br/_json/porto_hoje.asp?tipo=passageiros',
        'operadores': 'https://intranet.portodesantos.com.br/_json/operadores.asp'
    }

    resultados = {}

    for nome_df, url in urls.items():
        try:
            # Enviar a solicitação GET para a API
            response = requests.get(url)

            # Verificar o status da resposta
            if response.status_code == 200:
                # Tentar carregar os dados JSON
                try:
                    data = json.loads(response.text)
                except json.JSONDecodeError:
                    print(f'Erro ao decodificar JSON para a URL {url}. Dados recebidos não estão no formato esperado.')
                    resultados[nome_df] = None
                    continue

                # Normalizar os dados JSON em um DataFrame
                df = pd.json_normalize(data)
                resultados[nome_df] = df
            else:
                # Log de erro se a resposta da requisição não for 200 OK
                print(f'Erro na requisição para a URL {url}. Status code: {response.status_code}. Mensagem: {response.text}')
                resultados[nome_df] = None

        except Exception as e:
            # Log de exceção em caso de erro na requisição
            print(f'Exceção ao obter dados da URL {url}. Erro: {str(e)}')
            resultados[nome_df] = None

    return resultados

# Obter dados de várias APIs
resultados_dados = obter_dados_multiplos()

for nome_df, df in resultados_dados.items():
    if df is not None:
        print(f'Dados para {nome_df} recebidos com sucesso.')
    else:
        print(f'Falha na obtenção dos dados para {nome_df}.')

# Criação de um novo DataFrame com os campos desejados no formato previsto pela ABTRA / DTE
def transformar_aviso_chegada(df):
    aviso_chegada = pd.DataFrame()
    aviso_chegada['Viagem'] = df['viagem'].apply(lambda x: x if pd.notnull(x) else None)
    aviso_chegada['Navio'] = df['imo'].apply(lambda x: str(int(x)).zfill(8) if pd.notnull(x) else None)
    aviso_chegada['Nome Navio'] = df['nomenavio'].apply(lambda x: str(x)[:25] if pd.notnull(x) else None)
    aviso_chegada['Data'] = pd.to_datetime(df['data'], format='%d/%m/%Y', errors='coerce').dt.strftime('%d%m%Y')
    aviso_chegada['Hora'] = df['periodo'].apply(lambda x: x.split('/')[0].replace(':', '') if pd.notnull(x) and '/' in x else None)
    aviso_chegada['Tipo de Escala'] = df['manobra'].apply(lambda x: 'E' if x == 'ATRACACAO' else 'D' if x == 'DESCARGA' else 'C' if x == 'CABOTAGEM' else 'P' if x == 'PASSAGEIRO' else None)

    return aviso_chegada

# Acessar o DataFrame correto do dicionário `resultados_dados`
df_atracacoes_programadas = resultados_dados.get('atracacoes_programadas')

# Verificar se o DataFrame foi carregado com sucesso e aplicar a transformação
if df_atracacoes_programadas is not None:
    df_aviso_chegada = transformar_aviso_chegada(df_atracacoes_programadas)
    print('Dados transformados para aviso de chegada.')

    # Salvar o DataFrame como um arquivo CSV
    df_aviso_chegada.to_csv('aviso_chegada.csv', index=False, sep=';')
    print('Arquivo CSV salvo como "aviso_chegada.csv".')
else:
    print('Dados para atracacoes_programadas não estão disponíveis.')

"""
Os arquivos CSV gerados por este script seguirão os processos internos da ABTRA para garantir a conformidade e a integração adequada
com os sistemas e procedimentos da empresa. Especificamente:

1. **Estrutura dos Arquivos CSV:** Os arquivos CSV serão formatados de acordo com as especificações técnicas definidas pela ABTRA, incluindo
   o uso de delimitadores, codificação de caracteres e estrutura das colunas. Essa padronização facilita a integração com os sistemas existentes
   e assegura a consistência dos dados.

2. **Processamento de Dados:** Após a geração dos arquivos CSV, serão aplicados processos internos da ABTRA para validação, verificação e
   transformação dos dados conforme as normas estabelecidas. Isso inclui a verificação de integridade dos dados, o tratamento de inconsistências
   e a aplicação de regras de negócios específicas.

3. **Armazenamento e Acesso:** Os arquivos CSV gerados serão armazenados em locais específicos definidos pela ABTRA, com acesso controlado
   para garantir a segurança e a confidencialidade das informações. O caminho e o formato de armazenamento serão configurados de acordo com
   as políticas internas da empresa.

4. **Auditoria e Relatórios:** A ABTRA implementará processos de auditoria para monitorar e registrar a criação e o uso dos arquivos CSV,
   garantindo a conformidade com os requisitos regulatórios e internos. Relatórios de auditoria serão gerados conforme necessário para
   acompanhamento e revisão.

5. **Recomendação de Execução:** É recomendável que o script seja executado a cada 1 hora, ou na periodicidade que for julgada pertinente, para
   garantir que os dados estejam sempre atualizados e disponíveis para os processos subsequentes. Essa frequência ajuda a manter a integridade
   dos dados e a garantir que as informações mais recentes sejam refletidas nos arquivos CSV gerados.

Certifique-se de que todos os arquivos CSV gerados estejam em conformidade com as diretrizes da ABTRA antes de prosseguir com o carregamento
no sistema DTE.
"""

# somente um exemplo de ingestão

# # Substitua pelos detalhes do seu banco de dados
# DATABASE_TYPE = 'mysql'  # ou 'mssql' para SQL Server
# DBAPI = 'mysqlconnector'  # ou 'pyodbc' para SQL Server
# ENDPOINT = 'localhost'  # Endereço do banco de dados
# USER = 'username'  # Seu usuário
# PASSWORD = 'password'  # Sua senha
# PORT = 3306  # Porta para MySQL ou 1433 para SQL Server
# DATABASE = 'your_database'

# if DATABASE_TYPE == 'mysql':
#     engine = create_engine(f'{DATABASE_TYPE}+{DBAPI}://{USER}:{PASSWORD}@{ENDPOINT}:{PORT}/{DATABASE}')
# else:
#     engine = create_engine(f'{DATABASE_TYPE}+{DBAPI}://{USER}:{PASSWORD}@{ENDPOINT}:{PORT}/{DATABASE}')

def validar_carga_banco(nome_tabela, arquivo_csv, engine):
    """
    Função para validar a carga dos dados no banco de dados.
    A idéia aqui é validar a INGESTÃO NO BANCO para direcionar
    o responsável em caso de falha. Não é data quality.

    Args:
        nome_tabela (str): Nome da tabela no banco de dados.
        arquivo_csv (str): Caminho do arquivo CSV gerado.
        engine (Engine): Conexão com o banco de dados SQLAlchemy.

    Returns:
        bool: True se a validação for bem-sucedida, False caso contrário.
    """
    # Carregar o CSV em um DataFrame
    df_csv = pd.read_csv(arquivo_csv)

    # Contar o número de registros no CSV
    num_registros_csv = len(df_csv)

    # Consultar o número de registros na tabela do banco de dados
    with engine.connect() as conn:
        query = f"SELECT COUNT(*) FROM {nome_tabela}"
        result = conn.execute(query)
        num_registros_banco = result.scalar()

    # Comparar o número de registros
    if num_registros_csv == num_registros_banco:
        print(f'Validação bem-sucedida: {num_registros_csv} registros encontrados no CSV e no banco de dados.')
        return True
    else:
        print(f'Validação falhou: {num_registros_csv} registros no CSV, mas {num_registros_banco} no banco de dados.')
        return False

def verificar_viagens_no_banco(df, nome_tabela, engine):
    """
    Função para verificar se todas as viagens no DataFrame estão presentes no banco de dados.
    Aqui é feita a validação quantitativa de dados na ABTRA e no sistema DTE.
    Caso existam divergências, responsáveis serão acionados via service desk.

    Args:
        df (DataFrame): DataFrame contendo dados das APIs com uma coluna 'Viagem'.
        nome_tabela (str): Nome da tabela no banco de dados onde as viagens devem estar presentes.
        engine (Engine): Conexão com o banco de dados SQLAlchemy.

    Returns:
        list: Lista de viagens que estão no DataFrame mas não estão presentes no banco de dados.
    """
    # Obter viagens distintas do DataFrame
    viagens_df = df['Viagem'].dropna().unique()

    # Obter viagens distintas do banco de dados
    query = f"SELECT DISTINCT Viagem FROM {nome_tabela}"
    with engine.connect() as conn:
        result = conn.execute(query)
        viagens_banco = {row['Viagem'] for row in result}

    # Verificar quais viagens do DataFrame estão faltando no banco de dados
    viagens_faltando = [viagem for viagem in viagens_df if viagem not in viagens_banco]

    return viagens_faltando

# Exemplo de uso:
# Suponha que df seja o DataFrame obtido das APIs e 'nome_tabela' seja a tabela de destino no banco
nome_tabela_destino = 'nome_da_tabela_destino'  # Altere para o nome real da tabela
viagens_faltando = verificar_viagens_no_banco(df_aviso_chegada, nome_tabela_destino, engine)

if viagens_faltando:
    print(f'As seguintes viagens estão no DataFrame mas não estão no banco de dados: {viagens_faltando}')
else:
    print('Todas as viagens do DataFrame estão presentes no banco de dados.')

# def verificar_consistencia_dados(df):
    """
    Função para verificar a consistência dos dados e emitir alertas em caso de divergências.

    Args:
        df (DataFrame): DataFrame contendo os dados a serem verificados.

    Returns:
        dict: Dicionário com relatórios de alertas encontrados.
    """
    alertas = []

    # 1. Verificar campos nulos importantes
    campos_importantes = ['Viagem', 'Navio', 'Data', 'Hora']
    for campo in campos_importantes:
        if df[campo].isnull().any():
            alertas.append(f'Atenção: Campo {campo} contém valores nulos.')

    # 2. Verificar a consistência das datas
    try:
        df['Data'] = pd.to_datetime(df['Data'], format='%d%m%Y', errors='coerce')
    except Exception as e:
        alertas.append(f'Erro ao converter as datas. Detalhes: {str(e)}')

    if df['Data'].isnull().any():
        alertas.append('Atenção: Existem datas inválidas ou ausentes após a conversão.')

    # Verificar se há datas fora de um intervalo esperado
    data_minima = '2022-01-01'
    data_maxima = pd.Timestamp.today().strftime('%Y-%m-%d')
    datas_divergentes = df[(df['Data'] < data_minima) | (df['Data'] > data_maxima)]
    if not datas_divergentes.empty:
        alertas.append(f'Atenção: Existem datas fora do intervalo entre {data_minima} e {data_maxima}.')

    # 3. Verificar a consistência da Hora (assumindo que a hora deve ter 4 dígitos no formato HHMM)
    horas_invalidas = df[~df['Hora'].str.match(r'^\d{4}$', na=False)]
    if not horas_invalidas.empty:
        alertas.append(f'Atenção: Existem valores de hora inválidos. Total: {len(horas_invalidas)}')

    # Retornar o relatório de alertas
    if not alertas:
        return "Os dados estão consistentes. Nenhum alerta encontrado."
    else:
        return {"alertas": alertas}

# Exemplo de uso:
## lembre-se que esse dataframe foi gerado no início do processo e está sendo reaproveitado.
relatorio_alertas = verificar_consistencia_dados(df_aviso_chegada)
print(relatorio_alertas)